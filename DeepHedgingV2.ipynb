{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging V2 â€” Black-Scholes, Merton & Heston\n",
    "\n",
    "Pipeline complet de Deep Hedging avec le package `deep_hedging/`.\n",
    "\n",
    "### Nouveautes V2 vs V1\n",
    "| Feature | V1 | V2 |\n",
    "|---|---|---|\n",
    "| Features d'etat | 2 (S_rel, time) | 6 (log-moneyness, time, delta_{t-1}, vol realisee, BS delta, d1) |\n",
    "| Architectures | MLP uniquement | MLP + LSTM (recurrent) |\n",
    "| Mondes | BS, Merton | BS, Merton, **Heston** (vol stochastique) |\n",
    "| Payoffs | Call/Put | Call, Put, **Asian, Straddle, Lookback** |\n",
    "| Loss | CVaR empirique | CVaR + **OCE parametrique** (seuil appris) |\n",
    "| Metriques | 7 | **10** (+ Hedging Error, Sharpe PnL, Cost/Payoff) |\n",
    "| Baselines | Delta BS | Delta BS + **No Hedge** |\n",
    "| LR scheduler | Non | **ReduceLROnPlateau** |\n",
    "| Action clipping | Non | **Oui** (plus/moins 2.0) |\n",
    "\n",
    "### Pipeline\n",
    "1. Configuration\n",
    "2. Visualisation des trajectoires (BS, Merton, Heston)\n",
    "3. Training MLP et LSTM (BS puis Merton)\n",
    "4. Evaluation multi-scenarios (6 scenarios)\n",
    "5. Analyse de risque (VaR, CVaR, KDE, QQ-plot)\n",
    "6. Tableau de synthese colore (10 metriques)\n",
    "7. Comparaison MLP vs LSTM\n",
    "8. Bonus : payoffs exotiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import replace\n",
    "\n",
    "from deep_hedging import (\n",
    "    DEVICE, DTYPE, N_FEATURES,\n",
    "    DeepHedgingConfig, MarketConfig, TrainingConfig, RandomConfig,\n",
    "    SimpleWorldBS, SimpleWorldMerton, SimpleWorldHeston,\n",
    "    DeepHedgingEnv, PolicyMLP, PolicyLSTM, DeltaBSPolicy,\n",
    "    MonetaryUtility, OCEUtility,\n",
    "    train_deep_hedging,\n",
    "    evaluate_strategies_env_world, build_comparison_table,\n",
    "    RiskMetrics,\n",
    "    plot_training_history, plot_gains_hist, plot_payoff_vs_gains,\n",
    "    plot_simulated_paths, traffic_light_style,\n",
    ")\n",
    "\n",
    "print(f\"Device         : {DEVICE}\")\n",
    "print(f\"Features / step: {N_FEATURES}\")\n",
    "print(f\"Dtype          : {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DeepHedgingConfig()\n",
    "\n",
    "print(\"=== Marche ===\")\n",
    "print(f\"  S0={cfg.market.S0}, sigma={cfg.market.sigma}, K={cfg.market.K}, T={cfg.market.T}\")\n",
    "print(f\"  n_steps={cfg.market.n_steps}, cost_s={cfg.market.cost_s}\")\n",
    "print()\n",
    "print(\"=== Merton ===\")\n",
    "print(f\"  lambda_jump={cfg.market.lambda_jump}, mu_J={cfg.market.mu_J}, sigma_J={cfg.market.sigma_J}\")\n",
    "print()\n",
    "print(\"=== Heston ===\")\n",
    "print(f\"  kappa={cfg.market.heston_kappa}, theta={cfg.market.heston_theta}\")\n",
    "print(f\"  xi={cfg.market.heston_xi}, rho={cfg.market.heston_rho}, v0={cfg.market.heston_v0}\")\n",
    "print()\n",
    "print(\"=== Training ===\")\n",
    "print(f\"  epochs={cfg.training.n_epochs}, batch_size={cfg.training.batch_size}\")\n",
    "print(f\"  lr={cfg.training.lr}, CVaR alpha={cfg.training.cvar_alpha}\")\n",
    "print(f\"  train paths={cfg.market.n_paths_train:,}, val paths={cfg.market.n_paths_val:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisation des trajectoires simulees\n",
    "\n",
    "Comparaison visuelle des trois mondes avec les memes parametres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_bs = SimpleWorldBS(cfg.market)\n",
    "data_bs = world_bs.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_bs[\"S\"], n_paths_to_plot=20)\n",
    "print(f\"Payoff call moyen : {data_bs['payoff'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Merton Jump-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_merton = SimpleWorldMerton(cfg.market)\n",
    "data_merton = world_merton.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_merton[\"S\"], n_paths_to_plot=20)\n",
    "print(f\"Payoff call moyen : {data_merton['payoff'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Heston Stochastic Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_heston = SimpleWorldHeston(cfg.market)\n",
    "data_heston = world_heston.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_heston[\"S\"], n_paths_to_plot=20)\n",
    "print(f\"Payoff call moyen : {data_heston['payoff'].mean():.2f}\")\n",
    "\n",
    "# Trajectoires de variance v_t\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i in range(20):\n",
    "    ax.plot(data_heston[\"v\"][i], linewidth=0.8, alpha=0.7)\n",
    "ax.axhline(cfg.market.heston_theta, color=\"red\", linestyle=\"--\",\n",
    "           label=f\"theta={cfg.market.heston_theta}\")\n",
    "ax.set_xlabel(\"Time steps\")\n",
    "ax.set_ylabel(\"Variance $v_t$\")\n",
    "ax.set_title(\"Heston - Variance Paths (mean-reverting)\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrainement Deep Hedging\n",
    "\n",
    "On entraine 4 modeles :\n",
    "- MLP sous Black-Scholes\n",
    "- LSTM sous Black-Scholes\n",
    "- MLP sous Merton\n",
    "- LSTM sous Merton\n",
    "\n",
    "Chacun utilise les **6 features enrichies**, le **LR scheduler**, et l'**action clipping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MLP - Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_bs = DeepHedgingConfig(\n",
    "    market=replace(cfg.market, use_jumps=False, payoff_type=\"call\"),\n",
    "    training=cfg.training,\n",
    "    random=cfg.random,\n",
    "    device=cfg.device,\n",
    "    dtype=cfg.dtype,\n",
    ")\n",
    "\n",
    "policy_mlp_bs = PolicyMLP(\n",
    "    d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0\n",
    ")\n",
    "print(f\"MLP : {sum(p.numel() for p in policy_mlp_bs.parameters())} params\")\n",
    "\n",
    "res_mlp_bs = train_deep_hedging(\n",
    "    cfg_bs, policy_mlp_bs,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_bs.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "plot_training_history(res_mlp_bs[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LSTM - Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_lstm_bs = PolicyLSTM(\n",
    "    d_in=N_FEATURES, d_hidden=32, n_layers=1, dropout=0.0, clip=2.0\n",
    ")\n",
    "print(f\"LSTM : {sum(p.numel() for p in policy_lstm_bs.parameters())} params\")\n",
    "\n",
    "res_lstm_bs = train_deep_hedging(\n",
    "    cfg_bs, policy_lstm_bs,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_bs.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "plot_training_history(res_lstm_bs[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MLP - Merton (avec sauts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_merton = DeepHedgingConfig(\n",
    "    market=replace(cfg.market, use_jumps=True, payoff_type=\"call\"),\n",
    "    training=cfg.training,\n",
    "    random=cfg.random,\n",
    "    device=cfg.device,\n",
    "    dtype=cfg.dtype,\n",
    ")\n",
    "\n",
    "policy_mlp_merton = PolicyMLP(\n",
    "    d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0\n",
    ")\n",
    "\n",
    "res_mlp_merton = train_deep_hedging(\n",
    "    cfg_merton, policy_mlp_merton,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_merton.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "plot_training_history(res_mlp_merton[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 LSTM - Merton (avec sauts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_lstm_merton = PolicyLSTM(\n",
    "    d_in=N_FEATURES, d_hidden=32, n_layers=1, dropout=0.0, clip=2.0\n",
    ")\n",
    "\n",
    "res_lstm_merton = train_deep_hedging(\n",
    "    cfg_merton, policy_lstm_merton,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_merton.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "plot_training_history(res_lstm_merton[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation multi-scenarios\n",
    "\n",
    "Six scenarios pour mesurer performance et robustesse :\n",
    "\n",
    "| Scenario | Train | Test | Mesure |\n",
    "|---|---|---|---|\n",
    "| MLP BS->BS | BS | BS | Performance in-sample |\n",
    "| LSTM BS->BS | BS | BS | LSTM vs MLP |\n",
    "| MLP BS->Merton | BS | Merton | Robustesse aux sauts |\n",
    "| MLP Merton->Merton | Merton | Merton | Performance in-sample |\n",
    "| LSTM Merton->Merton | Merton | Merton | LSTM vs MLP |\n",
    "| MLP Merton->Heston | Merton | Heston | Cross-model |\n",
    "\n",
    "Chaque evaluation inclut **No Hedge**, **Delta BS**, **Deep Hedging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MLP : BS -> BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_bs_bs = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_mlp_bs[\"policy\"],\n",
    "    world_class=SimpleWorldBS,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs,\n",
    ")\n",
    "print(\"=== MLP : BS -> BS ===\")\n",
    "print(f\"  CVaR Deep    : {eval_mlp_bs_bs['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta   : {eval_mlp_bs_bs['cvar_delta']:.4f}\")\n",
    "print(f\"  CVaR No Hedge: {eval_mlp_bs_bs['cvar_no_hedge']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_bs_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LSTM : BS -> BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lstm_bs_bs = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_lstm_bs[\"policy\"],\n",
    "    world_class=SimpleWorldBS,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs,\n",
    ")\n",
    "print(\"=== LSTM : BS -> BS ===\")\n",
    "print(f\"  CVaR Deep  : {eval_lstm_bs_bs['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta : {eval_lstm_bs_bs['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_lstm_bs_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 MLP : BS -> Merton (robustesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_bs_merton = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_mlp_bs[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs_merton,\n",
    ")\n",
    "print(\"=== MLP : BS -> Merton ===\")\n",
    "print(f\"  CVaR Deep    : {eval_mlp_bs_merton['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta   : {eval_mlp_bs_merton['cvar_delta']:.4f}\")\n",
    "print(f\"  CVaR No Hedge: {eval_mlp_bs_merton['cvar_no_hedge']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_bs_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 MLP : Merton -> Merton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_merton_merton = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_mlp_merton[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== MLP : Merton -> Merton ===\")\n",
    "print(f\"  CVaR Deep  : {eval_mlp_merton_merton['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta : {eval_mlp_merton_merton['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_merton_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 LSTM : Merton -> Merton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lstm_merton_merton = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_lstm_merton[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== LSTM : Merton -> Merton ===\")\n",
    "print(f\"  CVaR Deep  : {eval_lstm_merton_merton['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta : {eval_lstm_merton_merton['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_lstm_merton_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 MLP : Merton -> Heston (cross-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_merton_heston = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_mlp_merton[\"policy\"],\n",
    "    world_class=SimpleWorldHeston,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== MLP : Merton -> Heston ===\")\n",
    "print(f\"  CVaR Deep    : {eval_mlp_merton_heston['cvar_deep']:.4f}\")\n",
    "print(f\"  CVaR Delta   : {eval_mlp_merton_heston['cvar_delta']:.4f}\")\n",
    "print(f\"  CVaR No Hedge: {eval_mlp_merton_heston['cvar_no_hedge']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_merton_heston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse de risque detaillee\n",
    "\n",
    "Focus sur **Merton -> Merton** : comparaison Delta BS / MLP / LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = RiskMetrics(alpha=cfg.training.cvar_alpha)\n",
    "\n",
    "gains_delta = eval_mlp_merton_merton[\"gains_delta\"]\n",
    "gains_mlp   = eval_mlp_merton_merton[\"gains_deep\"]\n",
    "gains_lstm  = eval_lstm_merton_merton[\"gains_deep\"]\n",
    "\n",
    "for name, g in [(\"Delta BS\", gains_delta), (\"Deep MLP\", gains_mlp), (\"Deep LSTM\", gains_lstm)]:\n",
    "    s = rm.summary(g)\n",
    "    print(f\"{name:>10s} | Mean={s['Mean']:+.2f}  Std={s['Std']:.2f}  VaR={s['VaR']:.2f}  CVaR={s['CVaR']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Histogrammes avec VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.plot_hist_with_var(gains_delta, title=\"Delta Hedging - Merton\")\n",
    "rm.plot_hist_with_var(gains_mlp,  title=\"Deep Hedging MLP - Merton\")\n",
    "rm.plot_hist_with_var(gains_lstm, title=\"Deep Hedging LSTM - Merton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 KDE : comparaisons deux a deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.plot_kde(gains_delta, gains_mlp, label_a=\"Delta\", label_b=\"Deep MLP\")\n",
    "rm.plot_kde(gains_delta, gains_lstm, label_a=\"Delta\", label_b=\"Deep LSTM\")\n",
    "rm.plot_kde(gains_mlp, gains_lstm, label_a=\"MLP\", label_b=\"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 QQ-Plots vs distribution normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.plot_qq(gains_mlp, title=\"Deep MLP - QQ-plot vs Normal\")\n",
    "rm.plot_qq(gains_lstm, title=\"Deep LSTM - QQ-plot vs Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Queue gauche (tail risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.plot_left_tail(gains_delta, gains_mlp, label_a=\"Delta\", label_b=\"Deep MLP\")\n",
    "rm.plot_left_tail(gains_delta, gains_lstm, label_a=\"Delta\", label_b=\"Deep LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tableau de synthese\n",
    "\n",
    "Comparaison coloree (traffic light) : 3 strategies x 6 scenarios x 10 metriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {\n",
    "    \"MLP BS->BS\":           (cfg_bs,     eval_mlp_bs_bs),\n",
    "    \"LSTM BS->BS\":          (cfg_bs,     eval_lstm_bs_bs),\n",
    "    \"MLP BS->Merton\":       (cfg_bs,     eval_mlp_bs_merton),\n",
    "    \"MLP Merton->Merton\":   (cfg_merton, eval_mlp_merton_merton),\n",
    "    \"LSTM Merton->Merton\":  (cfg_merton, eval_lstm_merton_merton),\n",
    "    \"MLP Merton->Heston\":   (cfg_merton, eval_mlp_merton_heston),\n",
    "}\n",
    "\n",
    "all_tables = []\n",
    "for scenario_name, (c, ev) in scenarios.items():\n",
    "    t = build_comparison_table(c, ev).copy()\n",
    "    t.insert(0, \"Scenario\", scenario_name)\n",
    "    all_tables.append(t)\n",
    "\n",
    "summary_df = pd.concat(all_tables, axis=0)\n",
    "summary_df = summary_df.reset_index().rename(columns={\"index\": \"Strategy\"})\n",
    "summary_df = summary_df.set_index([\"Scenario\", \"Strategy\"])\n",
    "\n",
    "styled = (\n",
    "    summary_df.astype(float)\n",
    "    .style\n",
    "    .apply(traffic_light_style, axis=None)\n",
    "    .format(\"{:.4f}\")\n",
    ")\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison directe MLP vs LSTM\n",
    "\n",
    "Resume des performances cles pour les deux architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    \"Scenario\": [\"BS->BS\", \"BS->BS\", \"Merton->Merton\", \"Merton->Merton\"],\n",
    "    \"Architecture\": [\"MLP\", \"LSTM\", \"MLP\", \"LSTM\"],\n",
    "    \"CVaR Deep\": [\n",
    "        eval_mlp_bs_bs[\"cvar_deep\"],\n",
    "        eval_lstm_bs_bs[\"cvar_deep\"],\n",
    "        eval_mlp_merton_merton[\"cvar_deep\"],\n",
    "        eval_lstm_merton_merton[\"cvar_deep\"],\n",
    "    ],\n",
    "    \"Std Deep\": [\n",
    "        eval_mlp_bs_bs[\"std_deep\"],\n",
    "        eval_lstm_bs_bs[\"std_deep\"],\n",
    "        eval_mlp_merton_merton[\"std_deep\"],\n",
    "        eval_lstm_merton_merton[\"std_deep\"],\n",
    "    ],\n",
    "    \"CVaR Delta (ref)\": [\n",
    "        eval_mlp_bs_bs[\"cvar_delta\"],\n",
    "        eval_lstm_bs_bs[\"cvar_delta\"],\n",
    "        eval_mlp_merton_merton[\"cvar_delta\"],\n",
    "        eval_lstm_merton_merton[\"cvar_delta\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data).set_index([\"Scenario\", \"Architecture\"])\n",
    "print(comp_df.round(4).to_string())\n",
    "print()\n",
    "print(\"Note: un CVaR plus eleve (moins negatif) = meilleur tail risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bonus : Deep Hedging sur payoffs exotiques\n",
    "\n",
    "Entrainement rapide du MLP sur 5 payoffs differents sous Black-Scholes :\n",
    "- **Call** : max(S_T - K, 0)\n",
    "- **Put** : max(K - S_T, 0)\n",
    "- **Straddle** : |S_T - K|\n",
    "- **Asian** : max(mean(S) - K, 0)  *(path-dependent)*\n",
    "- **Lookback** : max(max(S) - K, 0)  *(path-dependent)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exotic_results = {}\n",
    "\n",
    "for ptype in [\"call\", \"put\", \"straddle\", \"asian\", \"lookback\"]:\n",
    "    print(f\"\\n--- Payoff: {ptype.upper()} ---\")\n",
    "    cfg_exotic = DeepHedgingConfig(\n",
    "        market=replace(cfg.market, use_jumps=False, payoff_type=ptype,\n",
    "                       n_paths_train=50_000, n_paths_val=10_000),\n",
    "        training=replace(cfg.training, n_epochs=30, print_every=10),\n",
    "        random=cfg.random,\n",
    "        device=cfg.device,\n",
    "        dtype=cfg.dtype,\n",
    "    )\n",
    "\n",
    "    pol = PolicyMLP(d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0)\n",
    "    res = train_deep_hedging(cfg_exotic, pol, patience=5, use_scheduler=True)\n",
    "\n",
    "    ev = evaluate_strategies_env_world(\n",
    "        cfg_exotic, res[\"policy\"],\n",
    "        world_class=SimpleWorldBS,\n",
    "        n_paths_eval=10_000,\n",
    "        seed_eval=42,\n",
    "    )\n",
    "    exotic_results[ptype] = ev\n",
    "\n",
    "# Tableau recap\n",
    "exotic_summary = pd.DataFrame({\n",
    "    ptype: {\n",
    "        \"CVaR Deep\": ev[\"cvar_deep\"],\n",
    "        \"CVaR Delta\": ev[\"cvar_delta\"],\n",
    "        \"CVaR No Hedge\": ev[\"cvar_no_hedge\"],\n",
    "        \"Std Deep\": ev[\"std_deep\"],\n",
    "        \"Std Delta\": ev[\"std_delta\"],\n",
    "    }\n",
    "    for ptype, ev in exotic_results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n=== Recap payoffs exotiques ===\")\n",
    "print(exotic_summary.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"policy_mlp_bs\": res_mlp_bs[\"policy\"].state_dict(),\n",
    "    \"policy_lstm_bs\": res_lstm_bs[\"policy\"].state_dict(),\n",
    "    \"policy_mlp_merton\": res_mlp_merton[\"policy\"].state_dict(),\n",
    "    \"policy_lstm_merton\": res_lstm_merton[\"policy\"].state_dict(),\n",
    "    \"config_bs\": cfg_bs,\n",
    "    \"config_merton\": cfg_merton,\n",
    "    \"history_mlp_bs\": res_mlp_bs[\"history\"],\n",
    "    \"history_lstm_bs\": res_lstm_bs[\"history\"],\n",
    "    \"history_mlp_merton\": res_mlp_merton[\"history\"],\n",
    "    \"history_lstm_merton\": res_lstm_merton[\"history\"],\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"deep_hedging_models_v2.pt\")\n",
    "print(f\"Checkpoint sauvegarde : deep_hedging_models_v2.pt\")\n",
    "print(f\"Contient {len(checkpoint)} elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook V2 orchestre le pipeline complet de Deep Hedging enrichi :\n",
    "\n",
    "| Composant | Details |\n",
    "|---|---|\n",
    "| **Mondes** | Black-Scholes, Merton Jump-Diffusion, Heston Stochastic Vol |\n",
    "| **Architectures** | MLP (feed-forward) et LSTM (recurrent) |\n",
    "| **Features** | 6D enrichies : log-moneyness, time, prev_delta, realized_vol, BS delta, d1 |\n",
    "| **Losses** | CVaR empirique, entropique, OCE parametrique |\n",
    "| **Payoffs** | Call, Put, Straddle, Asian, Lookback |\n",
    "| **Evaluation** | 3 strategies x 6 scenarios x 10 metriques |\n",
    "| **Risk** | VaR, CVaR, KDE, QQ-plot, queue gauche |\n",
    "\n",
    "### Structure du package\n",
    "\n",
    "```\n",
    "deep_hedging/\n",
    "  __init__.py      # Public API\n",
    "  config.py        # BS + Merton + Heston + payoff_type\n",
    "  worlds.py        # BS, Merton, Heston + payoffs exotiques\n",
    "  env.py           # 6 features enrichies\n",
    "  policies.py      # MLP, LSTM, DeltaBS (avec clipping)\n",
    "  losses.py        # CVaR, entropique, OCE parametrique\n",
    "  training.py      # DataLoader + early stop + LR scheduler\n",
    "  evaluation.py    # No Hedge + Delta + Deep, 10 metriques\n",
    "  risk_metrics.py  # VaR, CVaR, histogrammes, KDE, QQ-plot\n",
    "  plotting.py      # Visualisations + traffic light\n",
    "  utils.py         # Fonctions utilitaires\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}