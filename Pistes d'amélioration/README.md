# Pistes d'am√©lioration ‚Äî Deep Hedging

> Bas√© sur l'analyse du repo de r√©f√©rence [`hansbuehler/deephedging`](https://github.com/hansbuehler/deephedging)
> et la litt√©rature Deep Hedging (Buehler et al., 2019).

---

## Table des mati√®res

1. [Impact fort ‚Äî Prioritaires](#-impact-fort--prioritaires)
2. [Impact moyen ‚Äî Recommand√©s](#-impact-moyen--recommand√©s)
3. [Quick wins ‚Äî Faciles](#-quick-wins--faciles)
4. [R√©sum√© prioris√©](#-r√©sum√©-prioris√©)

---

## üî¥ Impact fort ‚Äî Prioritaires

### 1. R√©seau r√©current (LSTM / GRU)

**Probl√®me** : Le `PolicyMLP` actuel traite chaque pas de temps de mani√®re ind√©pendante. Il ne peut pas m√©moriser l'historique des positions ni capturer des d√©pendances temporelles.

**Solution** : Buehler a ajout√© le support LSTM/GRU en f√©vrier 2023 dans son repo. Un r√©seau r√©current peut :
- M√©moriser les positions pass√©es et l'historique des prix
- S'adapter dynamiquement √† la trajectoire observ√©e
- Mieux performer sur les **options path-dependent** (asiatiques, barriers, forward-started)

**Impl√©mentation** :
- Cr√©er une classe `PolicyLSTM` dans `policies.py`
- Architecture : LSTM ‚Üí Linear ‚Üí sigmoid/tanh pour borner les actions
- Comparaison MLP vs LSTM sur les m√™mes sc√©narios

**R√©f√©rence** : `notebooks/trainer-recurrent-fwdstart.ipynb` dans le repo Buehler

---

### 2. Features d'√©tat enrichies

**Probl√®me** : L'environnement n'utilise que 2 features : `S_t / S‚ÇÄ` (prix relatif) et `t / T` (temps √©coul√©). C'est tr√®s limit√©.

**Solution** : Ajouter des features informatives dans `env.py` :

| Feature | Formule | Justification |
|---|---|---|
| **Moneyness** | `log(S_t / K)` | Plus informatif que S/S‚ÇÄ pour le hedging |
| **Position courante** | `Œ¥_{t-1}` | Le r√©seau doit savoir sa position pour ajuster |
| **Vol r√©alis√©e glissante** | `std(log-returns, window=10)` | Signal de r√©gime de vol |
| **Delta BS initial** | `Œî_BS(S_t, K, T-t)` | Feature-engineering de qualit√© |
| **Log-moneyness / vol‚àöœÑ** | `log(S/K) / (œÉ‚àöœÑ)` | Variable d1 standardis√©e |

**Impact** : Le r√©seau converge plus vite et atteint de meilleures performances avec des features pr√©-trait√©es.

---

### 3. Hedging multi-instruments

**Probl√®me** : Le projet ne hedge qu'avec le sous-jacent spot. Le repo Buehler supporte le hedging avec **spot + option ATM**.

**Solution** :
- Ajouter un 2e instrument de hedging : option ATM vanille
- Le r√©seau output 2 actions par pas de temps : `(Œî_spot, Œî_option)`
- Permet de capturer le risque **gamma** et **vega** directement

**Complexit√©** : N√©cessite de pricer l'option √† chaque step (Black-Scholes) et de g√©rer les retours `DH_t` pour chaque instrument.

---

## üü° Impact moyen ‚Äî Recommand√©s

### 4. Monde √† volatilit√© stochastique (Heston)

**Probl√®me** : BS = vol constante, Merton = vol constante + sauts. Aucun ne capture le **smile de volatilit√© dynamique**.

**Solution** : Impl√©menter le mod√®le de Heston :

$$dS_t = S_t \sqrt{v_t}\, dW_t^S$$
$$dv_t = \kappa(\theta - v_t)\,dt + \xi\sqrt{v_t}\,dW_t^v$$

avec corr√©lation $\rho$ entre $W^S$ et $W^v$.

**Impl√©mentation** : `SimpleWorldHeston` dans `worlds.py`

**R√©f√©rence** : Buehler utilise un monde avec stochastic vol + mean-reverting drift.

---

### 5. OCE param√©trique (Optimized Certainty Equivalent)

**Probl√®me** : Le CVaR actuel utilise un quantile fixe. L'OCE de Buehler optimise le seuil VaR conjointement avec la policy.

**Solution** : Le vrai OCE dual est :

$$\text{OCE}(X) = \sup_w \left\{ w + \mathbb{E}[u(X - w)] \right\}$$

o√π $w$ (le "VaR level") est un param√®tre `nn.Parameter` appris par gradient descent.

**Impact** : Convergence plus stable et loss plus semantiquement correcte.

---

### 6. Payoffs exotiques

**Probl√®me** : Le projet ne traite que les calls/puts europ√©ens.

**Solution** : Ajouter des payoffs dans `env.py` :

| Payoff | Formule | Int√©r√™t |
|---|---|---|
| **Put europ√©en** | `max(K - S_T, 0)` | Sym√©trie call/put |
| **Asiatique** | `max(SÃÑ - K, 0)` avec SÃÑ = moyenne | Path-dependent ‚Üí LSTM n√©cessaire |
| **Barrier (knock-out)** | `max(S_T - K, 0) ¬∑ ùüô{S_t < B ‚àÄt}` | Discontinuit√© ‚Üí challenge pour NN |
| **Straddle** | `|S_T - K|` | Couverture delta-neutre |
| **Forward-started** | `max(S_T/S_{T/2} - 1, 0)` | Ref Buehler notebooks |

---

### 7. Benchmarks suppl√©mentaires

**Probl√®me** : Le seul benchmark est le Delta BS. C'est insuffisant pour √©valuer le Deep Hedging.

**Solution** :

| Benchmark | Description |
|---|---|
| **Delta-Vega Hedging** | Delta BS + hedge en vega (avec une option) |
| **No Hedge** | Aucun hedging (payoff brut) ‚Üí mesure la valeur ajout√©e |
| **Variance Optimal** | Couverture quadratique minimale (F√∂llmer-Schweizer) |
| **Delta Merton** | Delta ajust√© pour les sauts (si monde Merton) |

---

## üü¢ Quick wins ‚Äî Faciles

### 8. Learning Rate Scheduler

**Actuel** : Learning rate fixe √† 1e-3.

**Am√©lioration** :
```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=3
)
# ou
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=cfg.training.n_epochs
)
```

---

### 9. Action clipping / Contraintes sur les positions

**Probl√®me** : Aucune borne sur les actions ‚Üí le r√©seau peut prendre des positions irr√©alistes.

**Solution** :
```python
# Dans PolicyMLP.forward()
out = self.net(X_flat)
out = torch.clamp(out, -2.0, 2.0)  # position max ¬±200%
```

Buehler applique des bornes configurables sur les actions.

---

### 10. Initial Delta Hedge (agent s√©par√©)

**Probl√®me** : La premi√®re action (t=0) est fondamentalement diff√©rente des suivantes (on part de position 0).

**Solution** : Buehler apprend un **agent s√©par√© pour le hedge initial** :
- `init_delta_agent` : r√©seau small qui apprend Œ¥‚ÇÄ
- Le r√©seau principal apprend les ajustements ‚àÜŒ¥_t pour t ‚â• 1

Config : `config.gym.agent.init_delta.active = True`

---

### 11. M√©triques suppl√©mentaires

| M√©trique | Formule | Mesure |
|---|---|---|
| **Hedging Error** | `std(Gains ‚àí Payoff)` | Qualit√© du hedge (plus bas = mieux) |
| **Sharpe du PnL** | `mean(PnL) / std(PnL)` | Efficience risque/rendement |
| **Co√ªt / Payoff** | `mean(Cost) / mean(Payoff)` | Efficience en co√ªts |
| **P&L attribution** | D√©composition delta + gamma + vega + theta | Comprendre d'o√π vient le PnL |

---

## üìä R√©sum√© prioris√©

| # | Am√©lioration | Module | Difficult√© | Impact |
|---|---|---|---|---|
| 1 | Features enrichies (moneyness, vol, Œ¥_{t-1}) | `env.py` | ‚≠ê | ‚≠ê‚≠ê‚≠ê |
| 2 | LSTM/GRU Policy | `policies.py` | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| 3 | Action clipping + LR scheduler | `policies.py`, `training.py` | ‚≠ê | ‚≠ê‚≠ê |
| 4 | M√©triques suppl√©mentaires | `evaluation.py`, `risk_metrics.py` | ‚≠ê | ‚≠ê‚≠ê |
| 5 | Payoffs exotiques | `env.py` | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| 6 | Heston stochastic vol | `worlds.py` | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| 7 | Benchmarks (no hedge, delta-vega) | `evaluation.py` | ‚≠ê | ‚≠ê‚≠ê |
| 8 | OCE param√©trique | `losses.py` | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| 9 | Hedging multi-instruments | `env.py`, `policies.py` | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| 10 | Initial delta hedge agent | `policies.py` | ‚≠ê‚≠ê | ‚≠ê |

---

## Ordre d'impl√©mentation recommand√©

```
Phase 1 (Quick wins)           Phase 2 (Core)              Phase 3 (Avanc√©)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Features enrichies           ‚Ä¢ LSTM/GRU Policy           ‚Ä¢ Hedging multi-instruments
‚Ä¢ Action clipping              ‚Ä¢ Heston World              ‚Ä¢ OCE param√©trique
‚Ä¢ LR scheduler                 ‚Ä¢ Payoffs exotiques         ‚Ä¢ Initial delta agent
‚Ä¢ M√©triques suppl.             ‚Ä¢ Benchmarks suppl.
```
