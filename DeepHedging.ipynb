{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging — Black-Scholes, Merton & Heston\n",
    "\n",
    "Pipeline complet de Deep Hedging avec le package `deep_hedging/`.\n",
    "\n",
    "### Nouveautes v2\n",
    "- **6 features enrichies** (log-moneyness, vol realisee, BS delta hint, ...)\n",
    "- **PolicyLSTM** (reseau recurrent) en plus du MLP\n",
    "- **Heston** (volatilite stochastique) en plus de BS et Merton\n",
    "- **Payoffs exotiques** : call, put, asian, straddle, lookback\n",
    "- **OCE parametrique** : loss differentiable avec seuil appris\n",
    "- **Metriques enrichies** : Hedging Error, Sharpe PnL, Cost/Payoff, No Hedge baseline\n",
    "\n",
    "### Pipeline\n",
    "1. Configuration\n",
    "2. Visualisation des trajectoires (BS, Merton, Heston)\n",
    "3. Training MLP et LSTM (BS puis Merton)\n",
    "4. Evaluation multi-scenarios\n",
    "5. Analyse de risque (VaR, CVaR, KDE, QQ-plot)\n",
    "6. Tableau de synthese colore\n",
    "7. Bonus : payoffs exotiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import replace\n",
    "\n",
    "from deep_hedging import (\n",
    "    DEVICE, DTYPE, N_FEATURES,\n",
    "    DeepHedgingConfig, MarketConfig, TrainingConfig, RandomConfig,\n",
    "    SimpleWorldBS, SimpleWorldMerton, SimpleWorldHeston,\n",
    "    DeepHedgingEnv, PolicyMLP, PolicyLSTM, DeltaBSPolicy,\n",
    "    MonetaryUtility, OCEUtility,\n",
    "    train_deep_hedging,\n",
    "    evaluate_strategies_env_world, build_comparison_table,\n",
    "    RiskMetrics,\n",
    "    plot_training_history, plot_gains_hist, plot_payoff_vs_gains,\n",
    "    plot_simulated_paths, traffic_light_style,\n",
    ")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Features par pas de temps: {N_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = DeepHedgingConfig()\n",
    "print(f\"Marche  : S0={cfg.market.S0}, sigma={cfg.market.sigma}, K={cfg.market.K}\")\n",
    "print(f\"Merton  : jumps={cfg.market.use_jumps}, lambda={cfg.market.lambda_jump}\")\n",
    "print(f\"Heston  : kappa={cfg.market.heston_kappa}, theta={cfg.market.heston_theta}, xi={cfg.market.heston_xi}, rho={cfg.market.heston_rho}\")\n",
    "print(f\"Training: epochs={cfg.training.n_epochs}, batch={cfg.training.batch_size}, lr={cfg.training.lr}\")\n",
    "print(f\"CVaR alpha={cfg.training.cvar_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisation des trajectoires simulees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_bs = SimpleWorldBS(cfg.market)\n",
    "data_bs = world_bs.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_bs[\"S\"], n_paths_to_plot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Merton Jump-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_merton = SimpleWorldMerton(cfg.market)\n",
    "data_merton = world_merton.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_merton[\"S\"], n_paths_to_plot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Heston Stochastic Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_heston = SimpleWorldHeston(cfg.market)\n",
    "data_heston = world_heston.simulate_paths(200, seed=42)\n",
    "plot_simulated_paths(data_heston[\"S\"], n_paths_to_plot=20)\n",
    "\n",
    "# Trajectoires de variance\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "for i in range(20):\n",
    "    ax.plot(data_heston[\"v\"][i], linewidth=0.8, alpha=0.7)\n",
    "ax.set_xlabel(\"Time steps\")\n",
    "ax.set_ylabel(\"Variance v_t\")\n",
    "ax.set_title(\"Heston — Variance Paths\")\n",
    "ax.grid(alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Deep Hedging\n",
    "\n",
    "### 4.1 Entrainement MLP sous Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_bs = DeepHedgingConfig(\n",
    "    market=replace(cfg.market, use_jumps=False, payoff_type=\"call\"),\n",
    "    training=cfg.training,\n",
    "    random=cfg.random,\n",
    "    device=cfg.device,\n",
    "    dtype=cfg.dtype,\n",
    ")\n",
    "\n",
    "# MLP avec 6 features enrichies\n",
    "policy_mlp_bs = PolicyMLP(d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0)\n",
    "\n",
    "res_mlp_bs = train_deep_hedging(\n",
    "    cfg_bs, policy_mlp_bs,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_bs.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "print(\"Training MLP BS termine.\")\n",
    "plot_training_history(res_mlp_bs[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Entrainement LSTM sous Black-Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_lstm_bs = PolicyLSTM(d_in=N_FEATURES, d_hidden=32, n_layers=1, dropout=0.0, clip=2.0)\n",
    "\n",
    "res_lstm_bs = train_deep_hedging(\n",
    "    cfg_bs, policy_lstm_bs,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_bs.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "print(\"Training LSTM BS termine.\")\n",
    "plot_training_history(res_lstm_bs[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrainement MLP sous Merton (sauts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_merton = DeepHedgingConfig(\n",
    "    market=replace(cfg.market, use_jumps=True, payoff_type=\"call\"),\n",
    "    training=cfg.training,\n",
    "    random=cfg.random,\n",
    "    device=cfg.device,\n",
    "    dtype=cfg.dtype,\n",
    ")\n",
    "\n",
    "policy_mlp_merton = PolicyMLP(d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0)\n",
    "\n",
    "res_mlp_merton = train_deep_hedging(\n",
    "    cfg_merton, policy_mlp_merton,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_merton.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "print(\"Training MLP Merton termine.\")\n",
    "plot_training_history(res_mlp_merton[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Entrainement LSTM sous Merton (sauts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_lstm_merton = PolicyLSTM(d_in=N_FEATURES, d_hidden=32, n_layers=1, dropout=0.0, clip=2.0)\n",
    "\n",
    "res_lstm_merton = train_deep_hedging(\n",
    "    cfg_merton, policy_lstm_merton,\n",
    "    utility=MonetaryUtility(kind=\"cvar\", alpha=cfg_merton.training.cvar_alpha),\n",
    "    patience=5, min_delta=1e-3, use_scheduler=True,\n",
    ")\n",
    "print(\"Training LSTM Merton termine.\")\n",
    "plot_training_history(res_lstm_merton[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation multi-scenarios\n",
    "\n",
    "Quatre scenarios principaux :\n",
    "- **BS → BS** : entrainement et test sous Black-Scholes\n",
    "- **BS → Merton** : entrainement BS, test avec sauts\n",
    "- **Merton → Merton** : entrainement et test avec sauts\n",
    "- **Merton → Heston** : entrainement Merton, test vol stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 MLP BS → BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_bs_bs = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_mlp_bs[\"policy\"],\n",
    "    world_class=SimpleWorldBS,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs,\n",
    ")\n",
    "print(\"=== MLP : BS -> BS ===\")\n",
    "print(f\"CVaR Deep  : {eval_mlp_bs_bs['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_mlp_bs_bs['cvar_delta']:.4f}\")\n",
    "print(f\"CVaR NoHedge: {eval_mlp_bs_bs['cvar_no_hedge']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_bs_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LSTM BS → BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lstm_bs_bs = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_lstm_bs[\"policy\"],\n",
    "    world_class=SimpleWorldBS,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs,\n",
    ")\n",
    "print(\"=== LSTM : BS -> BS ===\")\n",
    "print(f\"CVaR Deep  : {eval_lstm_bs_bs['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_lstm_bs_bs['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_lstm_bs_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 MLP BS → Merton (robustesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_bs_merton = evaluate_strategies_env_world(\n",
    "    cfg_bs, res_mlp_bs[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_bs.random.seed_eval_bs_merton,\n",
    ")\n",
    "print(\"=== MLP : BS -> Merton ===\")\n",
    "print(f\"CVaR Deep  : {eval_mlp_bs_merton['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_mlp_bs_merton['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_bs_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 MLP Merton → Merton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_merton_merton = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_mlp_merton[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== MLP : Merton -> Merton ===\")\n",
    "print(f\"CVaR Deep  : {eval_mlp_merton_merton['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_mlp_merton_merton['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_merton_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 LSTM Merton → Merton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lstm_merton_merton = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_lstm_merton[\"policy\"],\n",
    "    world_class=SimpleWorldMerton,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== LSTM : Merton -> Merton ===\")\n",
    "print(f\"CVaR Deep  : {eval_lstm_merton_merton['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_lstm_merton_merton['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_lstm_merton_merton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 MLP Merton → Heston (cross-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mlp_merton_heston = evaluate_strategies_env_world(\n",
    "    cfg_merton, res_mlp_merton[\"policy\"],\n",
    "    world_class=SimpleWorldHeston,\n",
    "    n_paths_eval=20_000,\n",
    "    seed_eval=cfg_merton.random.seed_eval_merton,\n",
    ")\n",
    "print(\"=== MLP : Merton -> Heston ===\")\n",
    "print(f\"CVaR Deep  : {eval_mlp_merton_heston['cvar_deep']:.4f}\")\n",
    "print(f\"CVaR Delta : {eval_mlp_merton_heston['cvar_delta']:.4f}\")\n",
    "plot_gains_hist(eval_mlp_merton_heston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse de risque detaillee\n",
    "\n",
    "Focus sur le scenario Merton → Merton (MLP vs LSTM vs Delta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = RiskMetrics(alpha=cfg.training.cvar_alpha)\n",
    "\n",
    "# MLP Merton->Merton\n",
    "gains_delta = eval_mlp_merton_merton[\"gains_delta\"]\n",
    "gains_mlp   = eval_mlp_merton_merton[\"gains_deep\"]\n",
    "gains_lstm  = eval_lstm_merton_merton[\"gains_deep\"]\n",
    "\n",
    "# Histogrammes avec VaR\n",
    "rm.plot_hist_with_var(gains_delta, title=\"Delta Hedging — Merton\")\n",
    "rm.plot_hist_with_var(gains_mlp,  title=\"Deep Hedging MLP — Merton\")\n",
    "rm.plot_hist_with_var(gains_lstm, title=\"Deep Hedging LSTM — Merton\")\n",
    "\n",
    "# KDE : Delta vs MLP vs LSTM\n",
    "rm.plot_kde(gains_delta, gains_mlp, label_a=\"Delta\", label_b=\"Deep MLP\")\n",
    "rm.plot_kde(gains_delta, gains_lstm, label_a=\"Delta\", label_b=\"Deep LSTM\")\n",
    "rm.plot_kde(gains_mlp, gains_lstm, label_a=\"MLP\", label_b=\"LSTM\")\n",
    "\n",
    "# QQ-Plot\n",
    "rm.plot_qq(gains_mlp, title=\"Deep MLP — QQ-plot vs Normal\")\n",
    "rm.plot_qq(gains_lstm, title=\"Deep LSTM — QQ-plot vs Normal\")\n",
    "\n",
    "# Queue gauche\n",
    "rm.plot_left_tail(gains_delta, gains_mlp, label_a=\"Delta\", label_b=\"Deep MLP\")\n",
    "rm.plot_left_tail(gains_delta, gains_lstm, label_a=\"Delta\", label_b=\"Deep LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tableau de synthese\n",
    "\n",
    "Comparaison coloree de tous les scenarios avec metriques enrichies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construire les tableaux\n",
    "scenarios = {\n",
    "    \"MLP BS->BS\": (cfg_bs, eval_mlp_bs_bs),\n",
    "    \"LSTM BS->BS\": (cfg_bs, eval_lstm_bs_bs),\n",
    "    \"MLP BS->Merton\": (cfg_bs, eval_mlp_bs_merton),\n",
    "    \"MLP Merton->Merton\": (cfg_merton, eval_mlp_merton_merton),\n",
    "    \"LSTM Merton->Merton\": (cfg_merton, eval_lstm_merton_merton),\n",
    "    \"MLP Merton->Heston\": (cfg_merton, eval_mlp_merton_heston),\n",
    "}\n",
    "\n",
    "all_tables = []\n",
    "for scenario_name, (c, ev) in scenarios.items():\n",
    "    t = build_comparison_table(c, ev).copy()\n",
    "    t.insert(0, \"Scenario\", scenario_name)\n",
    "    all_tables.append(t)\n",
    "\n",
    "summary_df = pd.concat(all_tables, axis=0)\n",
    "summary_df = summary_df.reset_index().rename(columns={\"index\": \"Strategy\"})\n",
    "summary_df = summary_df.set_index([\"Scenario\", \"Strategy\"])\n",
    "\n",
    "# Colonnes metriques\n",
    "metric_cols = [c for c in summary_df.columns]\n",
    "summary_metrics = summary_df[metric_cols].astype(float)\n",
    "\n",
    "# Affichage avec code couleur\n",
    "styled = (\n",
    "    summary_metrics\n",
    "    .style\n",
    "    .apply(traffic_light_style, axis=None)\n",
    "    .format(\"{:.4f}\")\n",
    ")\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison directe MLP vs LSTM\n",
    "\n",
    "Resume des performances par architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les metriques cles pour MLP vs LSTM\n",
    "comparison_data = {\n",
    "    \"Scenario\": [\"BS->BS\", \"BS->BS\", \"Merton->Merton\", \"Merton->Merton\"],\n",
    "    \"Architecture\": [\"MLP\", \"LSTM\", \"MLP\", \"LSTM\"],\n",
    "    \"CVaR Deep\": [\n",
    "        eval_mlp_bs_bs[\"cvar_deep\"],\n",
    "        eval_lstm_bs_bs[\"cvar_deep\"],\n",
    "        eval_mlp_merton_merton[\"cvar_deep\"],\n",
    "        eval_lstm_merton_merton[\"cvar_deep\"],\n",
    "    ],\n",
    "    \"Std Deep\": [\n",
    "        eval_mlp_bs_bs[\"std_deep\"],\n",
    "        eval_lstm_bs_bs[\"std_deep\"],\n",
    "        eval_mlp_merton_merton[\"std_deep\"],\n",
    "        eval_lstm_merton_merton[\"std_deep\"],\n",
    "    ],\n",
    "    \"CVaR Delta (ref)\": [\n",
    "        eval_mlp_bs_bs[\"cvar_delta\"],\n",
    "        eval_lstm_bs_bs[\"cvar_delta\"],\n",
    "        eval_mlp_merton_merton[\"cvar_delta\"],\n",
    "        eval_lstm_merton_merton[\"cvar_delta\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data).set_index([\"Scenario\", \"Architecture\"])\n",
    "print(comp_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bonus : Deep Hedging sur payoffs exotiques\n",
    "\n",
    "Test rapide du MLP sur differents payoffs (entrainement BS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exotic_results = {}\n",
    "\n",
    "for ptype in [\"call\", \"put\", \"straddle\", \"asian\", \"lookback\"]:\n",
    "    cfg_exotic = DeepHedgingConfig(\n",
    "        market=replace(cfg.market, use_jumps=False, payoff_type=ptype,\n",
    "                       n_paths_train=50_000, n_paths_val=10_000),\n",
    "        training=replace(cfg.training, n_epochs=30, print_every=10),\n",
    "        random=cfg.random,\n",
    "        device=cfg.device,\n",
    "        dtype=cfg.dtype,\n",
    "    )\n",
    "\n",
    "    pol = PolicyMLP(d_in=N_FEATURES, d_hidden=32, depth=2, dropout=0.1, clip=2.0)\n",
    "    res = train_deep_hedging(cfg_exotic, pol, patience=5)\n",
    "\n",
    "    ev = evaluate_strategies_env_world(\n",
    "        cfg_exotic, res[\"policy\"],\n",
    "        world_class=SimpleWorldBS,\n",
    "        n_paths_eval=10_000,\n",
    "        seed_eval=42,\n",
    "    )\n",
    "    exotic_results[ptype] = ev\n",
    "    print(f\"Payoff {ptype:>10s} | CVaR Deep={ev['cvar_deep']:.2f}, CVaR Delta={ev['cvar_delta']:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"Tous les payoffs exotiques traites.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"policy_mlp_bs\": res_mlp_bs[\"policy\"].state_dict(),\n",
    "    \"policy_lstm_bs\": res_lstm_bs[\"policy\"].state_dict(),\n",
    "    \"policy_mlp_merton\": res_mlp_merton[\"policy\"].state_dict(),\n",
    "    \"policy_lstm_merton\": res_lstm_merton[\"policy\"].state_dict(),\n",
    "    \"config_bs\": cfg_bs,\n",
    "    \"config_merton\": cfg_merton,\n",
    "    \"history_mlp_bs\": res_mlp_bs[\"history\"],\n",
    "    \"history_lstm_bs\": res_lstm_bs[\"history\"],\n",
    "    \"history_mlp_merton\": res_mlp_merton[\"history\"],\n",
    "    \"history_lstm_merton\": res_lstm_merton[\"history\"],\n",
    "}, \"deep_hedging_models.pt\")\n",
    "\n",
    "print(\"Modeles sauvegardes dans 'deep_hedging_models.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook orchestre le pipeline complet de Deep Hedging v2 :\n",
    "\n",
    "| Fonctionnalite | Details |\n",
    "|---|---|\n",
    "| **Mondes** | Black-Scholes, Merton Jump-Diffusion, Heston Stochastic Vol |\n",
    "| **Architectures** | MLP (feed-forward) et LSTM (recurrent) |\n",
    "| **Features** | 6D enrichies : log-moneyness, time, prev_delta, realized_vol, BS delta, d1 |\n",
    "| **Losses** | CVaR empirique, entropique, OCE parametrique |\n",
    "| **Payoffs** | Call, Put, Straddle, Asian, Lookback |\n",
    "| **Evaluation** | No Hedge + Delta BS + Deep Hedging, 10 metriques |\n",
    "| **Risk** | VaR, CVaR, KDE, QQ-plot, queue gauche |\n",
    "\n",
    "### Structure du package\n",
    "\n",
    "```\n",
    "deep_hedging/\n",
    "  __init__.py      # Public API\n",
    "  config.py        # Configurations + Heston params + payoff_type\n",
    "  worlds.py        # BS, Merton, Heston + payoffs exotiques\n",
    "  env.py           # 6 features enrichies\n",
    "  policies.py      # MLP, LSTM, DeltaBS\n",
    "  losses.py        # CVaR, entropique, OCE parametrique\n",
    "  training.py      # DataLoader + early stop + LR scheduler\n",
    "  evaluation.py    # No Hedge + Delta + Deep, 10 metriques\n",
    "  risk_metrics.py  # VaR, CVaR, KDE, QQ-plot\n",
    "  plotting.py      # Visualisations\n",
    "  utils.py         # Fonctions utilitaires\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}